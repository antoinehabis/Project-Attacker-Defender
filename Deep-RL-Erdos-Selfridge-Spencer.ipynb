{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement learning and the Attacker-Defender\n",
    "\n",
    "Antoine Habis & Geert-Jan Huizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "from random import choice, randint\n",
    "from tkinter import *\n",
    "\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from gym import error, spaces, utils\n",
    "from gym.utils import seeding\n",
    "from pulp import LpInteger, LpMinimize, LpProblem, LpVariable\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from stable_baselines import DQN, PPO2\n",
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines.common.policies import FeedForwardPolicy as FFP_common\n",
    "from stable_baselines.common.vec_env import DummyVecEnv\n",
    "from stable_baselines.deepq.policies import FeedForwardPolicy as FFP_DQ\n",
    "from stable_baselines.results_plotter import load_results, ts2xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP_DQN(FFP_DQ):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(MLP_DQN, self).__init__(*args, **kwargs,\n",
    "                                      layers=[300, 300],\n",
    "                                      layer_norm=False,\n",
    "                                      feature_extraction=\"mlp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def movingAverage(values, window):\n",
    "    weights = np.repeat(1.0, window) / window\n",
    "    return np.convolve(values, weights, 'valid')\n",
    "\n",
    "def plot_results(log_folder, title='Learning Curve'):\n",
    "    x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "    y = movingAverage(y, window=50)\n",
    "    x = x[len(x) - len(y):] # Truncate x\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_results(log_folders, legends, title='Learning Curve'):\n",
    "    for log_folder in log_folders:\n",
    "        x, y = ts2xy(load_results(log_folder), 'timesteps')\n",
    "        y = movingAverage(y, window=100)\n",
    "        x = x[len(x) - len(y):] # Truncate x\n",
    "        plt.plot(x, y)\n",
    "    plt.legend(legends)\n",
    "    plt.xlabel('Number of Timesteps')\n",
    "    plt.ylabel('Rewards')\n",
    "    plt.title(title + \" Smoothed\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a defender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defender(gym.Env):\n",
    "\n",
    "\n",
    "    def __init__(self, K, initial_potential, save_states=False):\n",
    "        self.state = None\n",
    "        self.save_states = save_states\n",
    "        self.states_saved = []\n",
    "        self.actions_saved = []\n",
    "        self.game_state = None\n",
    "        self.K = K\n",
    "        self.initial_potential = initial_potential\n",
    "        self.weights = np.power(2.0, [-(self.K - i) for i in range(self.K + 1)])\n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        self.action_space = spaces.Discrete(2)\n",
    "        self.observation_space= spaces.MultiDiscrete([10]* (2*K+2))\n",
    "        \n",
    "\n",
    "    def potential(self, A):\n",
    "        return np.sum(A*self.weights)\n",
    "\n",
    "    def split(self, A):\n",
    "        B = self.game_state - A\n",
    "        return A, B\n",
    "\n",
    "    def erase(self, A):\n",
    "        self.game_state -= A\n",
    "        self.game_state = np.insert(self.game_state[:-1], 0, 0)\n",
    "\n",
    "\n",
    "    def optimal_split(self, ratio = 0.5):\n",
    "        if (sum(self.game_state) == 1):\n",
    "            if (randint(1,100)<=50):\n",
    "                return self.game_state, [0]*(self.K+1)\n",
    "            else:\n",
    "                return [0]*(self.K+1), self.game_state\n",
    "            \n",
    "        else:\n",
    "            prob = LpProblem(\"Optimal split\",LpMinimize)\n",
    "            A = []\n",
    "            for i in range(self.K + 1):\n",
    "                A += LpVariable(str(i), 0, self.game_state[i], LpInteger)\n",
    "            prob += sum([2**(-(self.K - i)) * c for c, i in zip(A, range(self.K + 1))]) - ratio * self.potential(self.game_state), \"Objective function\"\n",
    "            prob += sum([2**(-(self.K - i)) * c for c, i in zip(A, range(self.K + 1))]) >= ratio * self.potential(self.game_state), \"Constraint\"\n",
    "            #prob.writeLP(\"test.lp\")\n",
    "            prob.solve()\n",
    "            Abis = [0]*(self.K+1)\n",
    "            for v in prob.variables():\n",
    "                Abis[int(v.name)] = round(v.varValue)\n",
    "            B = [z - a for z, a in zip(self.game_state, Abis)]\n",
    "            return Abis, B\n",
    "\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "\n",
    "    def attacker_play(self):\n",
    "        prob = 90 \n",
    "        if(randint(1,100)<=prob):\n",
    "            return self.optimal_split()\n",
    "        else:\n",
    "            ratios = [0.1, 0.2, 0.3, 0.4, 0.6, 0.7, 0.8, 0.9]\n",
    "            return self.optimal_split(ratio=choice(ratios))\n",
    "\n",
    "\n",
    "    def check(self):\n",
    "        \"\"\"Function to chek if the game is over or not.\n",
    "        Returns:\n",
    "            int -- If the game is not over returns 0, otherwise returns 1 if the defender won or -1 if the attacker won.\n",
    "        \"\"\"\n",
    "\n",
    "        if (sum(self.game_state) == 0):\n",
    "            return 1\n",
    "        elif (self.game_state[-1] >=1 ):\n",
    "            return -1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "    def step(self, target):\n",
    "        if self.save_states:\n",
    "            self.states_saved.append(self.state)\n",
    "            self.actions_saved.append(target)\n",
    "        \n",
    "        A = self.state[: self.K + 1]\n",
    "        B = self.state[self.K + 1 :]\n",
    "        if (target == 0):\n",
    "            self.erase(A)\n",
    "        else:\n",
    "            self.erase(B)\n",
    "        win = self.check()\n",
    "        if(win):\n",
    "            self.done = 1\n",
    "            self.reward = win\n",
    "\n",
    "        if self.done != 1:\n",
    "            A, B = self.attacker_play()\n",
    "            self.state = np.concatenate([A,B])\n",
    "\n",
    "        return self.state, self.reward, self.done, {}\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        self.game_state = self.random_start()\n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        A, B = self.attacker_play()\n",
    "        self.state = np.concatenate([A,B])\n",
    "        return self.state\n",
    "\n",
    "    def random_start(self):\n",
    "        self.game_state = [0] * (self.K + 1)\n",
    "        potential = 0\n",
    "        stop = False\n",
    "        while (potential < self.initial_potential and not stop):\n",
    "            possible = self.initial_potential - potential\n",
    "            upper = self.K - 1 #upper is K-1 because K represents the top of the matrix which means end of the game\n",
    "            while (2**(-(self.K-upper)) > possible):\n",
    "                upper -=1\n",
    "            if(upper < 0):\n",
    "                stop = True\n",
    "            else:\n",
    "                self.game_state[randint(0,upper)]+=1\n",
    "                potential = self.potential(self.game_state)\n",
    "        return self.game_state\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        for j in range(self.K + 1):\n",
    "            print(self.game_state[j], end = \" \")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defense:\n",
    "    def __init__(self, method, K=5, P=0.95, log_dir=\"/tmp/gym/\", save_states=False):\n",
    "        self.method = method\n",
    "\n",
    "        self.K = K\n",
    "        self.state_size = 2 * (self.K + 1)\n",
    "        self.action_size = 2\n",
    "        self.reward = []\n",
    "        self.save_states = save_states\n",
    "\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "        env = Defender(K, P, save_states=self.save_states)\n",
    "        env = Monitor(env, self.log_dir, allow_early_resets=True)\n",
    "        self.envs = DummyVecEnv([lambda: env])\n",
    "\n",
    "        self.model = DQN(MLP_DQN, self.envs, verbose=0)\n",
    "\n",
    "        self.best_mean_reward, self.n_steps = -np.inf, 0\n",
    "\n",
    "    def callback(self, _locals, _globals):\n",
    "        \"\"\"\n",
    "        Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n",
    "        :param _locals: (dict)\n",
    "        :param _globals: (dict)\n",
    "        \"\"\"\n",
    "\n",
    "        # Print stats every 1000 calls\n",
    "        if (self.n_steps + 1) % 1000 == 0:\n",
    "            # Evaluate policy performance\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                print(x[-1], 'timesteps')\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(\n",
    "                    self.best_mean_reward, mean_reward))\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "\n",
    "        self.n_steps += 1\n",
    "        return True\n",
    "\n",
    "    def learn(self, timesteps=10000):\n",
    "        self.model.learn(total_timesteps=timesteps, callback=self.callback)\n",
    "        print(\"======\\nLEARNING DONE\\n======\")\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.model.save(filename)\n",
    "        print(\"Model saved !\\n Filename:\", filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model = DQN.load(filename, policy=MLP_DQN)\n",
    "\n",
    "    def run(self, nb_episodes=1000):\n",
    "        self.nb_episodes = nb_episodes\n",
    "\n",
    "        for index_episode in range(nb_episodes):\n",
    "            state = self.envs.reset()\n",
    "            state = np.reshape(np.array(state), [1, self.state_size])\n",
    "            done = False\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action, _states = self.model.predict(state)\n",
    "                next_state, reward, done, _ = self.envs.step(action)\n",
    "                next_state = np.reshape(\n",
    "                    np.array(next_state), [1, self.state_size])\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "            if index_episode % 100 == 0:\n",
    "                print(\"Episode {}#; \\t Nb of steps: {}; \\t Reward: {}.\".format(\n",
    "                    index_episode, steps + 1, reward))\n",
    "            if index_episode > 0:\n",
    "                self.reward += [((self.reward[-1] * len(self.reward)\n",
    "                                  ) + reward) / (len(self.reward) + 1)]\n",
    "            else:\n",
    "                self.reward += [reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_def = \"/tmp/gym/10-05/\"\n",
    "defense = Defense('DQN', K=10, P=0.95, log_dir=log_dir_def)\n",
    "defense.learn(4000)\n",
    "plot_results(log_dir_def)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training an attacker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attacker(gym.Env):\n",
    "\n",
    "    def __init__(self, K, initial_potential):\n",
    "        self.state = None\n",
    "        self.K = K\n",
    "        self.initial_potential = initial_potential\n",
    "        self.weights = 2.0**np.arange(-self.K, 1)\n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        self.action_space = spaces.Discrete(self.K+1)\n",
    "        self.observation_space = spaces.MultiDiscrete(10*np.ones(K+1))\n",
    "\n",
    "    def potential(self, A):\n",
    "        return np.sum(A*self.weights)\n",
    "\n",
    "    def split(self, A):\n",
    "        B = self.state - A\n",
    "        return A, B\n",
    "\n",
    "    def erase(self, A):\n",
    "        self.state -= A\n",
    "        self.state = np.insert(self.state[:-1], 0, 0)\n",
    "\n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np_random(seed)\n",
    "        return [seed]\n",
    "\n",
    "    def defense_play(self, A, B):\n",
    "        if self.potential(A) >= self.potential(B):\n",
    "            self.erase(A)\n",
    "        else:\n",
    "            self.erase(B)\n",
    "\n",
    "    def check(self):\n",
    "        if sum(self.state) == 0:\n",
    "            return -1  # defender won\n",
    "        elif self.state[-1] >= 1:\n",
    "            return 1  # attacker won\n",
    "        else:\n",
    "            return 0  # game not over\n",
    "\n",
    "    def step(self, target):\n",
    "        A = np.zeros(self.K + 1)\n",
    "        A[:target] = self.state[:target]\n",
    "\n",
    "        B = np.zeros(self.K + 1)\n",
    "        B[target+1:] = self.state[target+1:]\n",
    "\n",
    "        for _ in range(int(self.state[target])):\n",
    "            if self.potential(A) > self.potential(B):\n",
    "                B[target] += 1\n",
    "            else:\n",
    "                A[target] += 1\n",
    "\n",
    "        self.defense_play(A, B)\n",
    "\n",
    "        self.reward = self.check()\n",
    "        self.done = np.abs(self.reward)  # 1 if someone won\n",
    "\n",
    "        return self.state, self.reward, self.done, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = self.random_start()\n",
    "        self.done = 0\n",
    "        self.reward = 0\n",
    "        return self.state\n",
    "\n",
    "    def random_start(self):\n",
    "        self.state = np.zeros(self.K+1)\n",
    "        potential = 0\n",
    "        upper = self.K-1\n",
    "        while potential < self.initial_potential and upper >= 0:\n",
    "            possible = self.initial_potential - potential\n",
    "            upper = self.K-1\n",
    "            while 2**(upper-self.K) > possible:\n",
    "                upper -= 1\n",
    "            if upper >= 0:\n",
    "                self.state[randint(0, upper)] += 1\n",
    "                potential = self.potential(self.state)\n",
    "        return self.state\n",
    "\n",
    "    def render(self):\n",
    "        for j in range(self.K + 1):\n",
    "            print(self.state[j], end=\" \")\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attack:\n",
    "    def __init__(self, method, K=5, P=0.95, log_dir=\"/tmp/gym_attack/\"):\n",
    "        self.method = method\n",
    "\n",
    "        self.K = K\n",
    "        self.state_size = 2 * (self.K + 1)\n",
    "        self.action_size = 2\n",
    "        self.reward = []\n",
    "\n",
    "        self.log_dir = log_dir\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "\n",
    "        env = Attacker(K, P)\n",
    "        env = Monitor(env, self.log_dir, allow_early_resets=True)\n",
    "        self.envs = DummyVecEnv([lambda: env])\n",
    "\n",
    "        self.model = DQN(MLP_DQN, self.envs, verbose=0)\n",
    "\n",
    "        self.best_mean_reward, self.n_steps = -np.inf, 0\n",
    "\n",
    "    def callback(self, _locals, _globals):\n",
    "        # Print stats every 1000 calls\n",
    "        if (self.n_steps + 1) % 1000 == 0:\n",
    "            # Evaluate policy performance\n",
    "            x, y = ts2xy(load_results(self.log_dir), 'timesteps')\n",
    "            if len(x) > 0:\n",
    "                mean_reward = np.mean(y[-100:])\n",
    "                print(x[-1], 'timesteps')\n",
    "                print(\"Best mean reward: {:.2f} - Last mean reward per episode: {:.2f}\".format(\n",
    "                    self.best_mean_reward, mean_reward))\n",
    "\n",
    "                # New best model, you could save the agent here\n",
    "                if mean_reward > self.best_mean_reward:\n",
    "                    self.best_mean_reward = mean_reward\n",
    "\n",
    "        self.n_steps += 1\n",
    "        return True\n",
    "\n",
    "    def learn(self, timesteps=10000):\n",
    "        self.model.learn(total_timesteps=timesteps, callback=self.callback)\n",
    "\n",
    "    def save(self, filename):\n",
    "        self.model.save(filename)\n",
    "\n",
    "    def load(self, filename):\n",
    "        self.model = DQN.load(filename, policy=MLP_DQN)\n",
    "\n",
    "    def run(self, nb_episodes=1000):\n",
    "        self.nb_episodes = nb_episodes\n",
    "\n",
    "        for index_episode in range(nb_episodes):\n",
    "            state = self.envs.reset()\n",
    "            done = False\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action, _states = self.model.predict(state)\n",
    "                next_state, reward, done, _ = self.envs.step(action)\n",
    "                next_state = np.array(next_state)\n",
    "                state = next_state\n",
    "                steps += 1\n",
    "            if index_episode % 100 == 0:\n",
    "                print(\"Episode {}#; \\t Nb of steps: {}; \\t Reward: {}.\".format(\n",
    "                    index_episode, steps + 1, reward))\n",
    "            if index_episode > 0:\n",
    "                self.reward += [((self.reward[-1] * len(self.reward)\n",
    "                                  ) + reward) / (len(self.reward) + 1)]\n",
    "            else:\n",
    "                self.reward += [reward]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_att = \"/tmp/gym_attack/10-101/\"\n",
    "attack = Attack('DQN', K=10, P=1.01, log_dir=log_dir_att)\n",
    "attack.learn(4000)\n",
    "plot_results(log_dir_att)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir_def = \"/tmp/gym/def/\"\n",
    "n = 4000\n",
    "clf = MLPClassifier(hidden_layer_sizes=(300, 300))\n",
    "proportion_correct = []\n",
    "for K in [5, 8, 10]:\n",
    "    defender = Defender(K, .95)\n",
    "    \n",
    "    defense = Defense('DQN', K=K, P=0.95, log_dir=log_dir_def, save_states=True)\n",
    "    defense.learn(n//3)\n",
    "    X_test = np.array(defense.envs.get_attr('states_saved')[0])\n",
    "    \n",
    "    y_test = np.zeros(n//3)\n",
    "    for i in range(n//3):\n",
    "        A, B = X_test[i,:K+1], X_test[i,K+1:]\n",
    "        if defender.potential(A) < defender.potential(B):\n",
    "            y_test[i] = 1\n",
    "    \n",
    "    defense = Defense('DQN', K=K, P=0.95, log_dir=log_dir_def, save_states=True)\n",
    "    defense.learn(n)\n",
    "    X_train = np.array(defense.envs.get_attr('states_saved')[0])\n",
    "    y_rl_train = defense.model.predict(X_train)[0]\n",
    "    y_rl_test = defense.model.predict(X_test)[0]\n",
    "    \n",
    "    y_train = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        A, B = X_train[i,:K+1], X_train[i,K+1:]\n",
    "        if defender.potential(A) < defender.potential(B):\n",
    "            y_train[i] = 1\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_sup_test = clf.predict(X_test)\n",
    "    \n",
    "    proportion_correct.append((1 - np.sum(np.abs(y_test - y_rl_test))/len(y_test), 1 - np.sum(np.abs(y_test - y_sup_test))/len(y_test)))\n",
    "    print(\"proportion correct\", proportion_correct[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([5, 8, 10], proportion_correct, marker='o')\n",
    "plt.legend(['RL', 'Supervised'])\n",
    "plt.title('Proportion of moves correct on test set of RL and Supervised Learning for varying K')\n",
    "plt.xlabel('K')\n",
    "plt.ylabel('Proportion correct')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a graphical interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 20  # number of stages\n",
    "N = 8  # number of points\n",
    "\n",
    "nb_iter = 5  # number of attack and defense\n",
    "\n",
    "field_width = 4\n",
    "size_window = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_graphic(state, window, Field):\n",
    "    field_width = np.max(state)\n",
    "    K = len(state)  # number of stages\n",
    "    size_window = 300\n",
    "    l, c = np.empty(0),np.empty(0)\n",
    "    for i in range(K):\n",
    "        l  = np.concatenate((l,np.ones(int(state[i]))*(i+1)))\n",
    "    for unique in (np.unique(l)):\n",
    "        c = np.concatenate((c, np.arange(list(l).count(unique))))\n",
    "    l = K-l\n",
    "    N = len(c)\n",
    "    v = min(scale_x, scale_y)\n",
    "    c1 = (c+1/2)*scale_x - (1/4)*v\n",
    "    c2 = c1 + (1/2)*v\n",
    "    l1 = (l+1/2)*scale_y - (1/4)*v\n",
    "    l2 = l1 + (1/2)*v\n",
    "    # dictionnary of the ovals\n",
    "    dic = {}\n",
    "    # display the points in the grid\n",
    "    for i in range(N):\n",
    "        rond = Field.create_oval(c1[i], l1[i], c2[i], l2[i], fill='black')\n",
    "        dic[str(i)] = rond\n",
    "        Field.update()\n",
    "    Coord = Label(window)\n",
    "    Coord.pack(pady='10px')\n",
    "    return dic, N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Play**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coord of the different points\n",
    "c = np.random.randint(low=0, high=field_width, size=N)\n",
    "l = np.random.randint(low=1, high=K, size=N)\n",
    "\n",
    "# construction of the window and the canvas\n",
    "window = Tk()\n",
    "window.resizable(width=False, height=False)\n",
    "window.title(\"Erdos-Selfridge-Spencer-Game\")\n",
    "Field = Canvas(window, height=size_window, width=size_window)\n",
    "Field.pack()\n",
    "\n",
    "scale_x = size_window//field_width\n",
    "scale_y = size_window//K\n",
    "\n",
    "carreau = [[Field.create_rectangle(i*scale_x, j*scale_y, (scale_x+1)*60, (scale_y+1)*60, fill=\"#FFFFFF\")\n",
    "            for i in range(field_width)] for j in range(K)]\n",
    "\n",
    "v = min(scale_x, scale_y)\n",
    "c1 = (c+1/2) * scale_x - (1/4)*v\n",
    "c2 = c1 + (1/2) * v\n",
    "l1 = (l + 1/2) * scale_y - (1/4)*v\n",
    "l2 = l1 + (1/2)*v\n",
    "# dictionnary of the ovals\n",
    "dic = {}\n",
    "\n",
    "# display the points in the grid\n",
    "for i in range(N):\n",
    "    rond = Field.create_oval(c1[i], l1[i], c2[i], l2[i], fill='black')\n",
    "    dic[str(i)] = rond\n",
    "Coord = Label(window)\n",
    "Coord.pack(pady='10px')\n",
    "\n",
    "\n",
    "points = np.arange(N)\n",
    "for i in range(nb_iter):\n",
    "    if len(points) > 1:\n",
    "        random.shuffle(points) ### here is the decision of A (how to do the partition)\n",
    "        cut = np.random.randint(1, len(points))\n",
    "        A = points[:cut]\n",
    "        B = points[cut:]\n",
    "        # Here we have to code the decision of the defender which partition to choose\n",
    "        # j'ai pris une decision random pour tester\n",
    "        u = np.random.randint(2)\n",
    "        if u == 1:\n",
    "            D, C = A, B  # C represent the points to keep and D the ones to delete\n",
    "        else:\n",
    "            D, C = B, A\n",
    "    else:\n",
    "        D = points\n",
    "        C = []\n",
    "    # Here is the code of the animation\n",
    "    for j in points:\n",
    "        if j in D:\n",
    "            Field.delete(window, dic[str(j)])\n",
    "            Field.update()\n",
    "        else:\n",
    "            for m in range(30):\n",
    "                Field.move(dic[str(j)], 0, -scale_y/30)\n",
    "                time.sleep(0.01)\n",
    "                Field.update()\n",
    "                \n",
    "    # stopping conditions of the game:\n",
    "    time.sleep(1)\n",
    "    points = C\n",
    "    if (np.prod(l[C] - (i+1)) == 0):  # stop if a point reached K\n",
    "        T = Text(window, height=2, width=30)\n",
    "        T.pack()\n",
    "        T.insert(END, \"The attacker won\")\n",
    "        mainloop()\n",
    "        break\n",
    "\n",
    "    elif (len(C) == 0):  # stop if ther is no more points\n",
    "        T = Text(window, height=2, width=30)\n",
    "        T.pack()\n",
    "        T.insert(END, \"The defender won\")\n",
    "        mainloop()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using a trained attacker**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential = 1.01\n",
    "\n",
    "attacker = Attacker(10, potential)\n",
    "#attack = trained model\n",
    "state = attacker.random_start()\n",
    "\n",
    "print(\"Initial state\")\n",
    "print(state)\n",
    "\n",
    "field_width = int(np.max(state))\n",
    "size_window = 300\n",
    "l, c = np.empty(0), np.empty(0)\n",
    "K = len(state)\n",
    "\n",
    "\n",
    "############## Creation of the background of the window (THE GRID)#############\n",
    "window = Tk()\n",
    "window.resizable(width=False, height=False)\n",
    "window.title(\"Erdos-Selfridge-Spencer-Game\")\n",
    "Field = Canvas(window, height=size_window, width=size_window)\n",
    "Field.pack()\n",
    "\n",
    "scale_x = size_window//field_width\n",
    "scale_y = size_window//K\n",
    "\n",
    "carreau = [[Field.create_rectangle(i*scale_x, j*scale_y, (scale_x+1)*60, (scale_y+1)*60, fill=\"#FFFFFF\")\n",
    "            for i in range(field_width)] for j in range(K)]\n",
    "######################## Creation of the initial dots ######################################\n",
    "dic, N = state_graphic(state, window, Field)\n",
    "T = Text(window, height=2, width=30)\n",
    "T.configure(font=('Ebrima', 20, \"bold\"))\n",
    "T.tag_configure(\"center\", justify='center')\n",
    "T.pack()\n",
    "T.insert(END, \"Let the game begin\")\n",
    "window.update()\n",
    "time.sleep(3)\n",
    "T.delete('1.0', END)\n",
    "window.update()\n",
    "##################################### The game begins: #####################################\n",
    "done = False\n",
    "steps = 0\n",
    "\n",
    "while not done:\n",
    "    action, _states = attack.model.predict(state)\n",
    "    print(\"Action :\", action)\n",
    "\n",
    "######################################TURN OF THE ATTACKER########################################\n",
    "    next_state, reward, done, _ = attacker.step(action)\n",
    "    T.insert(END, \"The attacker chooses partitions\")\n",
    "    window.update()\n",
    "    time.sleep(2)\n",
    "    T.delete('1.0', END)\n",
    "    window.update()\n",
    "\n",
    "#####################################TURN OF THE DEFENDER#########################################\n",
    "    T.insert(END, \"Turn of the defender\")\n",
    "    window.update()\n",
    "    time.sleep(2)\n",
    "    T.delete('1.0', END)\n",
    "    window.update()\n",
    "\n",
    "    for j in range(N):\n",
    "        Field.delete(window, dic[str(j)])\n",
    "        Field.update()\n",
    "    next_state = np.array(next_state)\n",
    "    dic, N = state_graphic(next_state, window, Field)\n",
    "    Field.update()\n",
    "    window.update()\n",
    "    state = next_state\n",
    "    steps += 1\n",
    "if reward == 1:\n",
    "    T.insert(END, \"The attacker won\")\n",
    "    window.update()\n",
    "    print(\"Attacker won in\", steps, \"step(s)\")\n",
    "else:\n",
    "    T.insert(END, \"The defender won\")\n",
    "    window.update()\n",
    "    print(\"Attacker lost in\", steps, \"step(s)\")\n",
    "mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
